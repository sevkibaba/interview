version: '3.8'

services:
  spark-master:
    image: bitnami/spark:3.5.1
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
    ports:
      - '8080:8080'  # Spark Master Web UI
      - '7077:7077'  # Spark Master RPC
    volumes:
      - ./advanced-solution/output:/data
      - ./advanced-solution/spark_aggregation_job.py:/app/spark_aggregation_job.py
      - ./models:/app/models
      - ./services:/app/services
      - ./requirements.txt:/app/requirements.txt
    extra_hosts:
      - "host.docker.internal:host-gateway"
    command: >
      bash -c "
        pip3 install --no-cache-dir -r /app/requirements.txt &&
        /opt/bitnami/spark/bin/spark-class org.apache.spark.deploy.master.Master --host spark-master --port 7077 --webui-port 8080
      "

  spark-worker-1:
    image: bitnami/spark:3.5.1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
      - SPARK_WORKER_HOST=spark-worker-1
      - SPARK_WORKER_PORT=8081
      - SPARK_WORKER_WEBUI_PORT=8081
    ports:
      - '8081:8081'  # Worker 1 Web UI
    volumes:
      - ./advanced-solution/output:/data
      - ./advanced-solution/spark_aggregation_job.py:/app/spark_aggregation_job.py
      - ./models:/app/models
      - ./services:/app/services
      - ./requirements.txt:/app/requirements.txt
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      - spark-master
    command: >
      bash -c "
        pip3 install --no-cache-dir -r /app/requirements.txt &&
        /opt/bitnami/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077 --host spark-worker-1 --port 8081 --webui-port 8081 --memory 2g --cores 2
      "

  spark-worker-2:
    image: bitnami/spark:3.5.1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
      - SPARK_WORKER_HOST=spark-worker-2
      - SPARK_WORKER_PORT=8082
      - SPARK_WORKER_WEBUI_PORT=8082
    ports:
      - '8082:8082'  # Worker 2 Web UI
    volumes:
      - ./advanced-solution/output:/data
      - ./advanced-solution/spark_aggregation_job.py:/app/spark_aggregation_job.py
      - ./models:/app/models
      - ./services:/app/services
      - ./requirements.txt:/app/requirements.txt
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      - spark-master
    command: >
      bash -c "
        pip3 install --no-cache-dir -r /app/requirements.txt &&
        /opt/bitnami/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077 --host spark-worker-2 --port 8082 --webui-port 8082 --memory 2g --cores 2
      "

  # Job submission service - for submitting Spark jobs to the cluster
  spark-submit:
    image: bitnami/spark:3.5.1
    environment:
      - SPARK_MASTER_URL=spark://spark-master:7077
    volumes:
      - ./advanced-solution/output:/data
      - ./advanced-solution/spark_aggregation_job.py:/app/spark_aggregation_job.py
      - ./models:/app/models
      - ./services:/app/services
      - ./requirements.txt:/app/requirements.txt
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      - spark-master
      - spark-worker-1
      - spark-worker-2
    command: >
      bash -c "
        pip3 install --no-cache-dir -r /app/requirements.txt &&
        echo 'Spark cluster is ready! Use the following command to submit jobs:' &&
        echo 'docker-compose -f docker-compose-cluster.yml exec spark-submit spark-submit --master spark://spark-master:7077 --py-files /app/models/models.py,/app/services/writer_service_adv_sol.py /app/spark_aggregation_job.py --start_batch 1 --end_batch 20 --input_dir /data --output_dir /data' &&
        tail -f /dev/null
      "


